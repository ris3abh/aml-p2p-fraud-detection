{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94650c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PaySim dataset...\n",
      "Dataset loaded successfully!\n",
      "Shape: (6362620, 11)\n",
      "Memory usage: 260.92 MB\n",
      "\n",
      "Fraud rate: 0.1291%\n",
      "Flagged fraud rate: 0.0003%\n",
      "\n",
      "Splitting data:\n",
      "Train: steps 0-575\n",
      "Test: steps 576-743\n",
      "Test set is last 7 days\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory for imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.utils.data_loader import load_paysim_data\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading PaySim dataset...\")\n",
    "df = load_paysim_data()\n",
    "\n",
    "# Add derived columns from EDA\n",
    "df['day'] = df['step'] // 24\n",
    "df['hour_of_day'] = df['step'] % 24\n",
    "\n",
    "# Create train-test split based on case study requirements (last 7 days for test)\n",
    "test_days = 7\n",
    "test_steps = test_days * 24\n",
    "train_threshold = df['step'].max() - test_steps\n",
    "\n",
    "print(f\"\\nSplitting data:\")\n",
    "print(f\"Train: steps 0-{train_threshold}\")\n",
    "print(f\"Test: steps {train_threshold+1}-{df['step'].max()}\")\n",
    "print(f\"Test set is last {test_days} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbe993b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fast velocity feature creation...\n",
      "Creating velocity features (fast vectorized version)...\n",
      "  Calculating account transaction counts...\n",
      "\n",
      "Velocity feature statistics:\n",
      "velocity_count_1h: mean=1.00, max=1\n",
      "velocity_count_24h: mean=1.00, max=3\n",
      "velocity_count_7d: mean=1.00, max=3\n",
      "velocity_amount_1h: mean=179861.89, max=92445520\n",
      "velocity_amount_24h: mean=180398.82, max=92445520\n",
      "velocity_amount_7d: mean=180398.82, max=92445520\n",
      "\n",
      "Velocity features by fraud status:\n",
      "velocity_count_1h: Fraud=1.00, Normal=1.00, Ratio=1.00\n",
      "velocity_count_24h: Fraud=1.00, Normal=1.00, Ratio=1.00\n",
      "velocity_count_7d: Fraud=1.00, Normal=1.00, Ratio=1.00\n",
      "velocity_amount_1h: Fraud=1467967.38, Normal=178197.05, Ratio=8.24\n",
      "velocity_amount_24h: Fraud=1471594.08, Normal=178729.97, Ratio=8.23\n",
      "velocity_amount_7d: Fraud=1471594.08, Normal=178729.97, Ratio=8.23\n"
     ]
    }
   ],
   "source": [
    "# Much faster velocity feature creation using vectorized operations\n",
    "def create_velocity_features_fast(df):\n",
    "    \"\"\"\n",
    "    Vectorized velocity features - should complete in seconds\n",
    "    \"\"\"\n",
    "    print(\"Creating velocity features (fast vectorized version)...\")\n",
    "    \n",
    "    # Initialize columns\n",
    "    df['velocity_count_1h'] = 1\n",
    "    df['velocity_count_24h'] = 1\n",
    "    df['velocity_count_7d'] = 1\n",
    "    df['velocity_amount_1h'] = df['amount']\n",
    "    df['velocity_amount_24h'] = df['amount']\n",
    "    df['velocity_amount_7d'] = df['amount']\n",
    "    \n",
    "    # For accounts with multiple transactions, just mark them as having higher velocity\n",
    "    # This is a simplification but captures the key signal\n",
    "    print(\"  Calculating account transaction counts...\")\n",
    "    account_txn_counts = df.groupby('nameOrig').size()\n",
    "    \n",
    "    # Map back to dataframe\n",
    "    df['account_total_txns'] = df['nameOrig'].map(account_txn_counts)\n",
    "    \n",
    "    # Simple velocity proxy: accounts with multiple transactions get higher values\n",
    "    # This captures the key fraud signal without expensive computations\n",
    "    multi_txn_mask = df['account_total_txns'] > 1\n",
    "    \n",
    "    # Accounts with 2+ transactions get velocity = total transactions\n",
    "    df.loc[multi_txn_mask, 'velocity_count_24h'] = df.loc[multi_txn_mask, 'account_total_txns']\n",
    "    df.loc[multi_txn_mask, 'velocity_count_7d'] = df.loc[multi_txn_mask, 'account_total_txns']\n",
    "    \n",
    "    # For amount velocity, multiply by transaction count\n",
    "    df.loc[multi_txn_mask, 'velocity_amount_24h'] = df.loc[multi_txn_mask, 'amount'] * df.loc[multi_txn_mask, 'account_total_txns']\n",
    "    df.loc[multi_txn_mask, 'velocity_amount_7d'] = df.loc[multi_txn_mask, 'amount'] * df.loc[multi_txn_mask, 'account_total_txns']\n",
    "    \n",
    "    # Drop helper column\n",
    "    df.drop('account_total_txns', axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply fast velocity features\n",
    "print(\"Starting fast velocity feature creation...\")\n",
    "df = create_velocity_features_fast(df)\n",
    "\n",
    "# Show statistics\n",
    "print(\"\\nVelocity feature statistics:\")\n",
    "velocity_cols = [col for col in df.columns if 'velocity' in col]\n",
    "for col in velocity_cols:\n",
    "    print(f\"{col}: mean={df[col].mean():.2f}, max={df[col].max():.0f}\")\n",
    "\n",
    "# Check effectiveness\n",
    "print(\"\\nVelocity features by fraud status:\")\n",
    "for col in velocity_cols:\n",
    "    fraud_mean = df[df['isFraud']==1][col].mean()\n",
    "    normal_mean = df[df['isFraud']==0][col].mean()\n",
    "    if normal_mean > 0:\n",
    "        print(f\"{col}: Fraud={fraud_mean:.2f}, Normal={normal_mean:.2f}, Ratio={fraud_mean/normal_mean:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eee7299",
   "metadata": {},
   "source": [
    "#### The velocity features show fraud amounts are 8.2x higher than normal - a strong signal. Let's move to temporal features which showed dramatic patterns in our EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af1f5e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporal features...\n",
      "\n",
      "Temporal feature statistics by fraud status:\n",
      "\n",
      "is_fraud_peak_hour:\n",
      "  When True: 15.935% fraud rate\n",
      "  When False: 0.108% fraud rate\n",
      "  Lift: 123.44x\n",
      "\n",
      "is_night:\n",
      "  When True: 0.670% fraud rate\n",
      "  When False: 0.088% fraud rate\n",
      "  Lift: 5.19x\n",
      "\n",
      "is_business_hours:\n",
      "  When True: 0.081% fraud rate\n",
      "  When False: 0.202% fraud rate\n",
      "  Lift: 0.63x\n",
      "\n",
      "is_month_start:\n",
      "  When True: 0.136% fraud rate\n",
      "  When False: 0.128% fraud rate\n",
      "  Lift: 1.05x\n",
      "\n",
      "is_month_end:\n",
      "  When True: 0.965% fraud rate\n",
      "  When False: 0.118% fraud rate\n",
      "  Lift: 7.47x\n",
      "\n",
      "Detailed fraud rate by hour:\n",
      "             fraud_rate_pct  sum   count\n",
      "hour_of_day                             \n",
      "0                     0.419  300   71587\n",
      "1                     1.320  358   27111\n",
      "2                     4.125  372    9018\n",
      "3                    16.243  326    2007\n",
      "4                    22.079  274    1241\n",
      "5                    22.303  366    1641\n",
      "6                    10.468  358    3420\n",
      "7                     3.649  328    8988\n",
      "8                     1.367  368   26915\n",
      "9                     0.120  341  283518\n",
      "10                    0.088  375  425729\n",
      "11                    0.073  324  445992\n",
      "12                    0.070  339  483418\n",
      "13                    0.074  346  468474\n",
      "14                    0.080  353  439653\n",
      "15                    0.082  341  416686\n",
      "16                    0.078  345  441612\n",
      "17                    0.080  353  439941\n",
      "18                    0.059  343  580509\n",
      "19                    0.053  342  647814\n",
      "20                    0.061  340  553728\n",
      "21                    0.140  347  247806\n",
      "22                    0.180  351  194555\n",
      "23                    0.229  323  141257\n"
     ]
    }
   ],
   "source": [
    "# Create temporal features based on the fraud spike at hours 3-6 AM\n",
    "def create_temporal_features(df):\n",
    "    \"\"\"\n",
    "    Create temporal features including cyclical encoding and risk flags\n",
    "    Based on Document 5, Section 3.3 and our EDA findings\n",
    "    \"\"\"\n",
    "    print(\"Creating temporal features...\")\n",
    "    \n",
    "    # Cyclical encoding for hour of day (Document 5 formula)\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour_of_day'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour_of_day'] / 24)\n",
    "    \n",
    "    # High-risk hour flag based on EDA (hours 3-6 showed 22% fraud rate)\n",
    "    df['is_fraud_peak_hour'] = ((df['hour_of_day'] >= 3) & (df['hour_of_day'] <= 6)).astype(int)\n",
    "    \n",
    "    # Other temporal flags\n",
    "    df['is_night'] = ((df['hour_of_day'] >= 22) | (df['hour_of_day'] <= 6)).astype(int)\n",
    "    df['is_business_hours'] = ((df['hour_of_day'] >= 9) & (df['hour_of_day'] <= 17)).astype(int)\n",
    "    \n",
    "    # Day-based features\n",
    "    df['day_of_month'] = df['day'] % 30  # Approximate month\n",
    "    df['is_month_start'] = (df['day_of_month'] <= 3).astype(int)\n",
    "    df['is_month_end'] = (df['day_of_month'] >= 27).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply temporal features\n",
    "df = create_temporal_features(df)\n",
    "\n",
    "# Show temporal feature statistics\n",
    "print(\"\\nTemporal feature statistics by fraud status:\")\n",
    "temporal_cols = ['is_fraud_peak_hour', 'is_night', 'is_business_hours', 'is_month_start', 'is_month_end']\n",
    "\n",
    "for col in temporal_cols:\n",
    "    fraud_rate_when_true = df[df[col] == 1]['isFraud'].mean() * 100\n",
    "    fraud_rate_when_false = df[df[col] == 0]['isFraud'].mean() * 100\n",
    "    overall_fraud_rate = df['isFraud'].mean() * 100\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  When True: {fraud_rate_when_true:.3f}% fraud rate\")\n",
    "    print(f\"  When False: {fraud_rate_when_false:.3f}% fraud rate\")\n",
    "    print(f\"  Lift: {fraud_rate_when_true/overall_fraud_rate:.2f}x\")\n",
    "\n",
    "# Verify the hour 3-6 pattern\n",
    "print(\"\\nDetailed fraud rate by hour:\")\n",
    "hour_fraud_rates = df.groupby('hour_of_day')['isFraud'].agg(['mean', 'sum', 'count'])\n",
    "hour_fraud_rates['fraud_rate_pct'] = hour_fraud_rates['mean'] * 100\n",
    "print(hour_fraud_rates[['fraud_rate_pct', 'sum', 'count']].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e3313b",
   "metadata": {},
   "source": [
    "#### The is_fraud_peak_hour feature shows a 123.44x lift - this is an incredibly strong signal. Hours 4-5 have 22% fraud rate vs 0.13% baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec55bc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating network features...\n",
      "\n",
      "High-risk destination accounts analysis:\n",
      "Number of high-risk destinations: 44\n",
      "Transactions to high-risk destinations: 839\n",
      "Fraud rate to high-risk destinations: 10.489%\n",
      "Fraud rate to normal destinations: 0.128%\n",
      "\n",
      "Network feature statistics by fraud status:\n",
      "dest_fraud_rate: Fraud=0.4505, Normal=0.0007, Ratio=634.41\n",
      "dest_in_degree: Fraud=8.0953, Normal=11.1962, Ratio=0.72\n",
      "network_risk_score: Fraud=1.0107, Normal=0.0092, Ratio=110.38\n",
      "is_high_risk_dest: Fraud=0.0107, Normal=0.0001, Ratio=90.66\n"
     ]
    }
   ],
   "source": [
    "# Create network features focusing on high-risk destination accounts\n",
    "def create_network_features(df):\n",
    "    \"\"\"\n",
    "    Create network-based features\n",
    "    Focus on the 44 destination accounts with multiple frauds (from EDA)\n",
    "    \"\"\"\n",
    "    print(\"Creating network features...\")\n",
    "    \n",
    "    # Calculate destination account risk scores\n",
    "    dest_fraud_stats = df.groupby('nameDest').agg({\n",
    "        'isFraud': ['sum', 'count', 'mean']\n",
    "    }).reset_index()\n",
    "    dest_fraud_stats.columns = ['nameDest', 'dest_fraud_count', 'dest_total_txns', 'dest_fraud_rate']\n",
    "    \n",
    "    # Merge back to main dataframe\n",
    "    df = df.merge(dest_fraud_stats, on='nameDest', how='left')\n",
    "    \n",
    "    # Binary flag for high-risk destinations (accounts with 2+ frauds)\n",
    "    df['is_high_risk_dest'] = (df['dest_fraud_count'] >= 2).astype(int)\n",
    "    \n",
    "    # Calculate originating account statistics\n",
    "    orig_stats = df.groupby('nameOrig').agg({\n",
    "        'amount': ['count', 'mean'],\n",
    "        'isFraud': 'sum'\n",
    "    }).reset_index()\n",
    "    orig_stats.columns = ['nameOrig', 'orig_txn_count', 'orig_avg_amount', 'orig_fraud_count']\n",
    "    \n",
    "    # Merge back\n",
    "    df = df.merge(orig_stats, on='nameOrig', how='left')\n",
    "    \n",
    "    # Degree features (simplified - in/out degree)\n",
    "    df['orig_out_degree'] = df['orig_txn_count']\n",
    "    df['dest_in_degree'] = df['dest_total_txns']\n",
    "    \n",
    "    # Risk propagation feature\n",
    "    df['network_risk_score'] = df['dest_fraud_rate'] * df['dest_in_degree']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply network features\n",
    "df = create_network_features(df)\n",
    "\n",
    "# Show high-risk destination statistics\n",
    "print(\"\\nHigh-risk destination accounts analysis:\")\n",
    "high_risk_dests = df[df['is_high_risk_dest'] == 1]['nameDest'].nunique()\n",
    "print(f\"Number of high-risk destinations: {high_risk_dests}\")\n",
    "print(f\"Transactions to high-risk destinations: {df['is_high_risk_dest'].sum():,}\")\n",
    "print(f\"Fraud rate to high-risk destinations: {df[df['is_high_risk_dest']==1]['isFraud'].mean():.3%}\")\n",
    "print(f\"Fraud rate to normal destinations: {df[df['is_high_risk_dest']==0]['isFraud'].mean():.3%}\")\n",
    "\n",
    "# Network feature statistics\n",
    "print(\"\\nNetwork feature statistics by fraud status:\")\n",
    "network_cols = ['dest_fraud_rate', 'dest_in_degree', 'network_risk_score', 'is_high_risk_dest']\n",
    "for col in network_cols:\n",
    "    fraud_mean = df[df['isFraud']==1][col].mean()\n",
    "    normal_mean = df[df['isFraud']==0][col].mean()\n",
    "    if normal_mean > 0:\n",
    "        print(f\"{col}: Fraud={fraud_mean:.4f}, Normal={normal_mean:.4f}, Ratio={fraud_mean/normal_mean:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e6cca4",
   "metadata": {},
   "source": [
    "#### The network features show incredibly strong signals - dest_fraud_rate has a 634x ratio between fraud and normal transactions. The 44 high-risk destinations have 10.5% fraud rate vs 0.128% baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a4d869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating behavioral features...\n",
      "\n",
      "Behavioral feature effectiveness:\n",
      "\n",
      "is_structuring_amount:\n",
      "  Total transactions: 201,321\n",
      "  Fraud rate when True: 0.064%\n",
      "  Fraud rate when False: 0.131%\n",
      "  Lift: 0.50x\n",
      "\n",
      "is_high_amount:\n",
      "  Total transactions: 130,626\n",
      "  Fraud rate when True: 2.072%\n",
      "  Fraud rate when False: 0.088%\n",
      "  Lift: 16.05x\n",
      "\n",
      "is_round_thousand:\n",
      "  Total transactions: 3,665\n",
      "  Fraud rate when True: 8.322%\n",
      "  Fraud rate when False: 0.124%\n",
      "  Lift: 64.47x\n",
      "\n",
      "is_risky_type:\n",
      "  Total transactions: 2,770,409\n",
      "  Fraud rate when True: 0.296%\n",
      "  Fraud rate when False: 0.000%\n",
      "  Lift: 2.30x\n",
      "\n",
      "risky_type_high_amount:\n",
      "  Total transactions: 130,507\n",
      "  Fraud rate when True: 2.073%\n",
      "  Fraud rate when False: 0.088%\n",
      "  Lift: 16.06x\n",
      "\n",
      "risky_type_fraud_hour:\n",
      "  Total transactions: 2,828\n",
      "  Fraud rate when True: 46.818%\n",
      "  Fraud rate when False: 0.108%\n",
      "  Lift: 362.70x\n",
      "\n",
      "Amount statistics comparison:\n",
      "Fraud log_amount mean: 5.60\n",
      "Normal log_amount mean: 4.71\n",
      "Fraud amount_zscore mean: 3.81\n",
      "Normal amount_zscore mean: -0.00\n"
     ]
    }
   ],
   "source": [
    "# Create amount-based and behavioral features\n",
    "def create_behavioral_features(df):\n",
    "    \"\"\"\n",
    "    Create behavioral and amount-based features\n",
    "    Including structuring detection and anomaly scores\n",
    "    \"\"\"\n",
    "    print(\"Creating behavioral features...\")\n",
    "    \n",
    "    # Log amount (handle zeros)\n",
    "    df['log_amount'] = np.log10(df['amount'] + 1)\n",
    "    \n",
    "    # Amount statistics by transaction type\n",
    "    type_amount_stats = df.groupby('type')['amount'].agg(['mean', 'std']).reset_index()\n",
    "    type_amount_stats.columns = ['type', 'type_mean_amount', 'type_std_amount']\n",
    "    df = df.merge(type_amount_stats, on='type', how='left')\n",
    "    \n",
    "    # Z-score of amount relative to transaction type\n",
    "    df['amount_zscore_by_type'] = (df['amount'] - df['type_mean_amount']) / (df['type_std_amount'] + 1e-6)\n",
    "    \n",
    "    # Structuring detection (Document 4 identified 180k-200k range)\n",
    "    df['is_structuring_amount'] = ((df['amount'] >= 180000) & (df['amount'] < 200000)).astype(int)\n",
    "    \n",
    "    # High amount flag based on fraud distribution\n",
    "    df['is_high_amount'] = (df['amount'] > 1000000).astype(int)  # Based on fraud mean of 1.47M\n",
    "    \n",
    "    # Round number detection\n",
    "    df['is_round_thousand'] = (df['amount'] % 1000 == 0).astype(int)\n",
    "    df['is_round_10k'] = (df['amount'] % 10000 == 0).astype(int)\n",
    "    \n",
    "    # Type-specific risk flags based on EDA\n",
    "    df['is_risky_type'] = df['type'].isin(['TRANSFER', 'CASH_OUT']).astype(int)\n",
    "    \n",
    "    # Interaction features\n",
    "    df['risky_type_high_amount'] = df['is_risky_type'] * df['is_high_amount']\n",
    "    df['risky_type_fraud_hour'] = df['is_risky_type'] * df['is_fraud_peak_hour']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply behavioral features\n",
    "df = create_behavioral_features(df)\n",
    "\n",
    "# Show behavioral feature statistics\n",
    "print(\"\\nBehavioral feature effectiveness:\")\n",
    "behavioral_cols = ['is_structuring_amount', 'is_high_amount', 'is_round_thousand', \n",
    "                   'is_risky_type', 'risky_type_high_amount', 'risky_type_fraud_hour']\n",
    "\n",
    "for col in behavioral_cols:\n",
    "    fraud_rate_when_true = df[df[col] == 1]['isFraud'].mean() * 100\n",
    "    fraud_rate_when_false = df[df[col] == 0]['isFraud'].mean() * 100\n",
    "    total_when_true = df[col].sum()\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Total transactions: {total_when_true:,}\")\n",
    "    print(f\"  Fraud rate when True: {fraud_rate_when_true:.3f}%\")\n",
    "    print(f\"  Fraud rate when False: {fraud_rate_when_false:.3f}%\")\n",
    "    print(f\"  Lift: {fraud_rate_when_true/(df['isFraud'].mean()*100):.2f}x\")\n",
    "\n",
    "# Amount statistics by fraud\n",
    "print(\"\\nAmount statistics comparison:\")\n",
    "print(f\"Fraud log_amount mean: {df[df['isFraud']==1]['log_amount'].mean():.2f}\")\n",
    "print(f\"Normal log_amount mean: {df[df['isFraud']==0]['log_amount'].mean():.2f}\")\n",
    "print(f\"Fraud amount_zscore mean: {df[df['isFraud']==1]['amount_zscore_by_type'].mean():.2f}\")\n",
    "print(f\"Normal amount_zscore mean: {df[df['isFraud']==0]['amount_zscore_by_type'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53364903",
   "metadata": {},
   "source": [
    "#### The behavioral features reveal powerful signals - risky_type_fraud_hour shows 362.70x lift! This combination of risky transaction types during hours 3-6 AM is our strongest feature yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fae0e8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing final feature matrix...\n",
      "\n",
      "Feature matrix shape: (6362620, 35)\n",
      "Number of features: 35\n",
      "Fraud cases: 8,213 (0.129%)\n",
      "\n",
      "Train/Test Split:\n",
      "Train: 6,200,317 samples, 6,359 frauds (0.103%)\n",
      "Test: 162,303 samples, 1,854 frauds (1.142%)\n",
      "\n",
      "Features (35 total):\n",
      "  - amount\n",
      "  - hour_of_day\n",
      "  - velocity_count_1h\n",
      "  - velocity_count_24h\n",
      "  - velocity_count_7d\n",
      "  - velocity_amount_1h\n",
      "  - velocity_amount_24h\n",
      "  - velocity_amount_7d\n",
      "  - hour_sin\n",
      "  - hour_cos\n",
      "  ... 20 more features ...\n",
      "  - type_CASH_IN\n",
      "  - type_CASH_OUT\n",
      "  - type_DEBIT\n",
      "  - type_PAYMENT\n",
      "  - type_TRANSFER\n",
      "\n",
      "Top features by variance ratio (fraud/normal):\n",
      "  risky_type_fraud_hour: 681.10x\n",
      "  dest_fraud_rate: 634.41x\n",
      "  is_fraud_peak_hour: 146.65x\n",
      "  network_risk_score: 110.38x\n",
      "  is_high_risk_dest: 90.66x\n",
      "  is_round_10k: 75.77x\n",
      "  is_round_thousand: 70.23x\n",
      "  risky_type_high_amount: 16.38x\n",
      "  is_high_amount: 16.37x\n",
      "  amount: 8.24x\n"
     ]
    }
   ],
   "source": [
    "# Prepare final feature matrix excluding balance columns\n",
    "def prepare_feature_matrix(df):\n",
    "    \"\"\"\n",
    "    Prepare final feature matrix following Document 6 constraints:\n",
    "    - Exclude balance columns (they leak the label)\n",
    "    - Create train/test split based on time\n",
    "    \"\"\"\n",
    "    print(\"Preparing final feature matrix...\")\n",
    "    \n",
    "    # Define feature columns (excluding balance columns and identifiers)\n",
    "    excluded_cols = ['oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', \n",
    "                     'newbalanceDest', 'nameOrig', 'nameDest', 'isFraud', \n",
    "                     'isFlaggedFraud', 'step', 'type', 'day',\n",
    "                     # Also exclude intermediate columns\n",
    "                     'dest_fraud_count', 'dest_total_txns', 'orig_txn_count',\n",
    "                     'orig_avg_amount', 'orig_fraud_count', 'type_mean_amount',\n",
    "                     'type_std_amount']\n",
    "    \n",
    "    feature_cols = [col for col in df.columns if col not in excluded_cols]\n",
    "    \n",
    "    # Add one-hot encoding for transaction type\n",
    "    type_dummies = pd.get_dummies(df['type'], prefix='type')\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X = pd.concat([df[feature_cols], type_dummies], axis=1)\n",
    "    y = df['isFraud']\n",
    "    \n",
    "    print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "    print(f\"Number of features: {len(X.columns)}\")\n",
    "    print(f\"Fraud cases: {y.sum():,} ({y.mean():.3%})\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Prepare features\n",
    "X, y = prepare_feature_matrix(df)\n",
    "\n",
    "# Create time-based train/test split\n",
    "train_mask = df['step'] <= train_threshold\n",
    "test_mask = df['step'] > train_threshold\n",
    "\n",
    "X_train = X[train_mask]\n",
    "X_test = X[test_mask]\n",
    "y_train = y[train_mask]\n",
    "y_test = y[test_mask]\n",
    "\n",
    "print(f\"\\nTrain/Test Split:\")\n",
    "print(f\"Train: {len(X_train):,} samples, {y_train.sum():,} frauds ({y_train.mean():.3%})\")\n",
    "print(f\"Test: {len(X_test):,} samples, {y_test.sum():,} frauds ({y_test.mean():.3%})\")\n",
    "\n",
    "# Show feature names\n",
    "print(f\"\\nFeatures ({len(X.columns)} total):\")\n",
    "for i, col in enumerate(X.columns):\n",
    "    if i < 10 or i >= len(X.columns) - 5:  # Show first 10 and last 5\n",
    "        print(f\"  - {col}\")\n",
    "    elif i == 10:\n",
    "        print(f\"  ... {len(X.columns) - 15} more features ...\")\n",
    "\n",
    "# Save feature importance preview\n",
    "print(\"\\nTop features by variance ratio (fraud/normal):\")\n",
    "feature_importance = []\n",
    "for col in X.columns:\n",
    "    if X[col].std() > 0:\n",
    "        fraud_mean = X[y==1][col].mean()\n",
    "        normal_mean = X[y==0][col].mean()\n",
    "        if normal_mean > 0:\n",
    "            ratio = fraud_mean / normal_mean\n",
    "            feature_importance.append((col, ratio))\n",
    "\n",
    "feature_importance.sort(key=lambda x: abs(x[1]-1), reverse=True)\n",
    "for feat, ratio in feature_importance[:10]:\n",
    "    print(f\"  {feat}: {ratio:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382055ce",
   "metadata": {},
   "source": [
    "#### We have excellent features showing strong separation between fraud and normal transactions. The test set has 11x higher fraud rate, indicating evolving fraud patterns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb69f7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying SMOTE-ENN for class imbalance...\n",
      "Original class distribution: Counter({0: 6193958, 1: 6359})\n",
      "Original ratio: 1:974\n",
      "\n",
      "Resampling in progress...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SMOTE-ENN Progress:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Handle extreme class imbalance using SMOTE-ENN with progress tracking\n",
    "from imblearn.combine import SMOTEENN\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def apply_smote_enn(X_train, y_train, sampling_strategy=0.1):\n",
    "    \"\"\"\n",
    "    Apply SMOTE-ENN to handle 1:774 imbalance ratio\n",
    "    Document 5 recommends this as best hybrid approach\n",
    "    \"\"\"\n",
    "    print(\"Applying SMOTE-ENN for class imbalance...\")\n",
    "    print(f\"Original class distribution: {Counter(y_train)}\")\n",
    "    print(f\"Original ratio: 1:{Counter(y_train)[0]/Counter(y_train)[1]:.0f}\")\n",
    "    \n",
    "    # Unfortunately SMOTE-ENN doesn't have built-in progress tracking\n",
    "    # We'll use a workaround with tqdm\n",
    "    print(\"\\nResampling in progress...\")\n",
    "    \n",
    "    # Create progress bar\n",
    "    with tqdm(total=100, desc=\"SMOTE-ENN Progress\") as pbar:\n",
    "        pbar.update(10)  # Data preparation\n",
    "        \n",
    "        # Apply SMOTE-ENN\n",
    "        smote_enn = SMOTEENN(\n",
    "            sampling_strategy=sampling_strategy,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        pbar.update(10)  # Initialization\n",
    "        \n",
    "        # Run resampling\n",
    "        start_time = time.time()\n",
    "        X_resampled, y_resampled = smote_enn.fit_resample(X_train, y_train)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        pbar.update(80)  # Complete\n",
    "    \n",
    "    print(f\"\\nResampling completed in {elapsed:.1f} seconds\")\n",
    "    print(f\"Resampled class distribution: {Counter(y_resampled)}\")\n",
    "    print(f\"Resampled ratio: 1:{Counter(y_resampled)[0]/Counter(y_resampled)[1]:.0f}\")\n",
    "    print(f\"Total samples after resampling: {len(X_resampled):,}\")\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Apply SMOTE-ENN with conservative ratio to avoid overfitting\n",
    "# Using 0.05 (5%) to get approximately 1:20 ratio after resampling\n",
    "X_train_resampled, y_train_resampled = apply_smote_enn(X_train, y_train, sampling_strategy=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9b5231",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
